<!DOCTYPE html>
<html>
  <head>
    <title>Improving disk read performance – Abhijeet Kandalkar – Software Engineer @ Igalia | C++ developer | Open Source contributor in Webkit/Chromium</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="When I started working on analyzing the startup time of a browser, I didn’t have an idea where to start. For me, Performance was a number game and trial-n-error approach. The challenge I was looking into is described in bug 1270977, in which the chrome binary hosted on two different partitions behaved differently.
" />
    <meta property="og:description" content="When I started working on analyzing the startup time of a browser, I didn’t have an idea where to start. For me, Performance was a number game and trial-n-error approach. The challenge I was looking into is described in bug 1270977, in which the chrome binary hosted on two different partitions behaved differently.
" />
    
    <meta name="author" content="Abhijeet Kandalkar" />

    
    <meta property="og:title" content="Improving disk read performance" />
    <meta property="twitter:title" content="Improving disk read performance" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Abhijeet Kandalkar - Software Engineer @ Igalia | C++ developer | Open Source contributor in Webkit/Chromium" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="/images/me.jpg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Abhijeet Kandalkar</a></h1>
            <p class="site-description">Software Engineer @ Igalia | C++ developer | Open Source contributor in Webkit/Chromium</p>
          </div>

          <nav>
            <a href="/os">OpenSource</a>
            <a href="/">CV</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Improving disk read performance</h1>

  <div class="entry">
    <p>When I started working on analyzing the startup time of a browser, I didn’t have an idea where to start. For me, Performance was a number game and trial-n-error approach. The challenge I was looking into is described in bug <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1270977">1270977</a>, in which the chrome binary hosted on two different partitions behaved differently.</p>

<table>
  <thead>
    <tr>
      <th>BROWSER</th>
      <th>LaCrOS</th>
      <th>Mount point</th>
      <th>Partition</th>
      <th>fs type</th>
      <th>comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>B1</td>
      <td>stateful</td>
      <td>/run/imageloader/lacros-*/chrome</td>
      <td>stateful</td>
      <td>squashfs</td>
      <td>downloaded</td>
    </tr>
    <tr>
      <td>B2</td>
      <td>deployedLacros</td>
      <td>/usr/local/lacros-chrome</td>
      <td>stateful</td>
      <td>ext4</td>
      <td>pushed by developer</td>
    </tr>
  </tbody>
</table>

<p>You can learn more about LaCrOS in my previous <a href="https://abhijeetk.github.io/What-is-LaCrOS/">blog</a>.
From the above table, the difference in filesystem was the first suspect. Squashfs is a compressed read-only file system for Linux.</p>

<p>To emulate the first launch, all caches are cleared using drop_caches as below.
<code class="language-plaintext highlighter-rouge">echo 3 &gt; /proc/sys/vm/drop_caches</code></p>

<h4 id="how-to-measure-a-file-system-read-time">How to measure a file system read time?</h4>

<p>For investigation and testing/benchmarking,  sequential read requests are emulated using a <code class="language-plaintext highlighter-rouge">dd</code> command and random read requests are emulated using <code class="language-plaintext highlighter-rouge">fio</code>(Flexible IO Tester)</p>

<p>For sequential read, B1 took around <strong>92MB/s</strong> while B2 took <strong>242MB/s</strong>.(Higher read speed means faster). Its a huge difference. <code class="language-plaintext highlighter-rouge">ext4</code> is faster than <code class="language-plaintext highlighter-rouge">squashfs</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>localhost ~ # echo 3 &gt; /proc/sys/vm/drop_caches
localhost ~ # time dd
if=/run/imageloader/lacros-dogfood-dev/99.0.4824.0/chrome of=/dev/null bs=4096
43910+1 records in
43910+1 records out
179859368 bytes (180 MB, 172 MiB) copied, 1.93517 s, 92.9 MB/s

real    0m1.951s
user    0m0.020s
sys 0m0.637s
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>localhost ~ # echo 3 &gt; /proc/sys/vm/drop_caches
localhost ~ # time dd if=/usr/local/lacros-chrome/chrome of=/dev/null bs=4096
43910+1 records in
43910+1 records out
179859368 bytes (180 MB, 172 MiB) copied, 0.744337 s, 242 MB/s

real    0m0.767s
user    0m0.056s
sys 0m0.285s

</code></pre></div></div>

<p>As Squashfs is a compressed file system, it first uncompresses the data and then performs a read. So definitely it’s going to be slower than ext4 but the difference(242 -92) was huge and unacceptable.</p>

<h4 id="can-we-do-better">Can we do better?</h4>

<p>The next step was to find some tools that could provide more information.</p>

<ul>
  <li>strace -  Linux utility to find out system calls that take more time or have blocking IO. But couldn’t get much information out of this analysis.</li>
  <li><a href="https://man7.org/linux/man-pages/man1/iostat.1.html">iostat</a> - This provides more insight information for input/output</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">$echo 3 &gt; /proc/sys/vm/drop_caches &amp;&amp; iostat -d -tc -xm 1 20</code></p>

<p>After running this command for both B1 and B2, it appears that B1 is not merging read requests. In other words, rrqm/s and %rrqm values are ZERO. Refer <a href="https://man7.org/linux/man-pages/man1/iostat.1.html">man-iostat</a></p>

<blockquote>
  <p>%rrqm  The percentage of read requests merged together
before being sent to the device.</p>

  <p>rrqm/s The number of read requests merged per second that
were queued to the device.</p>
</blockquote>

<p>The data to be read is scattered in different sectors of a different block. As a result of scattering, the read requests were not merged, so there could be a lot of read requests down to the block device.</p>

<p>Next step was to find out who is responsible for merging read requests.</p>

<h4 id="io-scheduler">IO scheduler</h4>

<p><a href="https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers">IO scheduler</a> optimizes disk access requests by merging I/O requests to similar locations on disk.
To get the current IO scheduler for a disk-device</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$/sys/block/&lt;disk device&gt;/queue/scheduler
</code></pre></div></div>

<p>for B1,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>localhost ~ # cat /sys/block/dm-1/queue/scheduler 
none
</code></pre></div></div>

<p>For B2, it uses the bfq scheduler</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>localhost ~ # cat /sys/block/mmcblk0/queue/scheduler 
mq-deadline kyber [bfq] none
</code></pre></div></div>

<p>As you can notice, B1 does not have scheduler. For B2, scheduler is <a href="https://docs.kernel.org/block/bfq-iosched.html">bfq</a>(Budget Fair Queueing).</p>

<p><a href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-tuning-io.html">SUSE</a> and <a href="https://www.ibm.com/docs/en/linux-on-systems?topic=linuxonibm/performance/tuneforsybase/ioschedulers.htm">IBM</a> has good documentation about IO scheduling.</p>

<p>It has been found so far that there are differences between file systems, read request merge patterns, and IO schedulers. But there is one more difference which we havent noticed yet.</p>

<p>If you look at how B1 is mounted, you will see that we are using a virtual device.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>localhost ~ # df -TH
Filesystem  Type      Size  Used  Avail  Use%  Mounted on
/dev/dm-1   squashfs  143M  143M      0  100%  /run/imageloader/lacros-dogfood-dev/99.0.4824.0
</code></pre></div></div>

<h4 id="a-twist-in-my-story">A twist in my story</h4>

<p>For B1, we are not directly communicating with the block device instead the requests are going through a virtual device using a device mapper.
As you might have noticed above, <code class="language-plaintext highlighter-rouge">dm-1</code> device mapper virtual device is used by B1.</p>

<p>Read more about a device-mapper and refer to the diagram on <a href="https://en.wikipedia.org/wiki/Device_mapper">wikipedia</a> page.</p>

<p>We are using a dm-verity as a virtual device.</p>
<blockquote>
  <p>Device-Mapper’s “verity” target provides transparent integrity checking of block devices using a cryptographic digest provided by the kernel crypto API. This target is read-only.</p>
</blockquote>

<p>From a security point of view, it makes sense to host your executable binary to a read-only files system that provides integrity checking.</p>

<p>There is a twist in that dm-verity does not have a scheduler, and scheduling is delegated to the scheduler of the underlying block device. The I/O will not be merged at the device-mapper layer, but at an underlying layer.</p>

<p><img src="https://notes.igalia.com/uploads/e41f5131-bb6a-4fe8-9b01-1082efdcbd9c.png" alt="" /></p>

<p>Several layers are different between B1 and B2, including partition encryption(dm-verity) and compression(squashfs).</p>

<p>In addition, we found that B1 defaults to using <code class="language-plaintext highlighter-rouge">--direct-io</code></p>

<h3 id="what-is-direct-io-">What is DIRECT IO ?</h3>

<p>Direct I/O is a feature of the <em>file system</em> whereby file reads and writes go directly from the applications to the storage device, bypassing the operating system read and write caches.</p>

<p>Although direct I/O can reduce CPU usage, using it typically results in the process taking longer to complete, <strong>especially for relatively <em>small</em> requests</strong>. This penalty is caused by the fundamental differences between normal cached I/O and direct I/O.</p>

<p>Every direct I/O read causes a synchronous read from the disk; unlike the normal cached I/O policy where the read may be satisfied from the cache. This can result in very <em>poor</em> performance if the data was likely to be in memory under the normal caching policy.</p>

<h3 id="is-it-possible-to-control-how-many-read-requests-are-made">Is it possible to control how many read requests are made?</h3>

<p>We decided to go deeper and analyze the block layer. In the block layer the request queue is handled, and I/O scheduling is done. To see how the flow of IO goes through the block layer, there is a suit of tools consisting of</p>

<ul>
  <li>blktrace (tracing the IO requests through the block layer),</li>
  <li>blkparse (parsing the output of blktrace to make it human readable),</li>
  <li>btrace (script to combine blktrace and blkparse, and</li>
  <li><a href="https://usermanual.wiki/Document/bttmanual.1495776143/view">btt</a> (a blktrace output post-processing tool)), among others.</li>
</ul>

<p>Below is the result of block-level parsing.</p>

<p>For B1,</p>

<table>
  <thead>
    <tr>
      <th>ALL</th>
      <th>MIN</th>
      <th>AVG</th>
      <th>MAX</th>
      <th>N</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Q2Q</td>
      <td>0.000000001</td>
      <td>0.000069861</td>
      <td>0.004097547</td>
      <td>94883</td>
    </tr>
    <tr>
      <td>Q2G</td>
      <td>0.000000242</td>
      <td>0.000000543</td>
      <td>0.000132620</td>
      <td>94884</td>
    </tr>
    <tr>
      <td>G2I</td>
      <td>0.000000613</td>
      <td>0.000001455</td>
      <td>0.003585980</td>
      <td>94884</td>
    </tr>
    <tr>
      <td>I2D</td>
      <td>0.000001399</td>
      <td>0.000002456</td>
      <td>0.003777911</td>
      <td>94884</td>
    </tr>
    <tr>
      <td>D2C</td>
      <td>0.000133042</td>
      <td>0.000176883</td>
      <td>0.001771526</td>
      <td>23721</td>
    </tr>
    <tr>
      <td>Q2C</td>
      <td>0.000136308</td>
      <td>0.000181337</td>
      <td>0.003968613</td>
      <td>23721</td>
    </tr>
  </tbody>
</table>

<p>For B2,</p>

<table>
  <thead>
    <tr>
      <th>ALL</th>
      <th>MIN</th>
      <th>AVG</th>
      <th>MAX</th>
      <th>N</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Q2Q</td>
      <td>0.000000001</td>
      <td>0.000016582</td>
      <td>0.003283451</td>
      <td>76171</td>
    </tr>
    <tr>
      <td>Q2G</td>
      <td>0.000000178</td>
      <td>0.000000228</td>
      <td>0.000016893</td>
      <td>76172</td>
    </tr>
    <tr>
      <td>G2I</td>
      <td>0.000000626</td>
      <td>0.000005904</td>
      <td>0.000587304</td>
      <td>76172</td>
    </tr>
    <tr>
      <td>I2D</td>
      <td>0.000002011</td>
      <td>0.000007582</td>
      <td>0.000861783</td>
      <td>76172</td>
    </tr>
    <tr>
      <td>D2C</td>
      <td>0.000004959</td>
      <td>0.000067955</td>
      <td>0.001275667</td>
      <td>19043</td>
    </tr>
    <tr>
      <td>Q2C</td>
      <td>0.000009132</td>
      <td>0.000081670</td>
      <td>0.001315828</td>
      <td>19043</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Number of read requests(N) are more for B1 than B2.
    <ul>
      <li>For B1, N = 94883 (partition with <code class="language-plaintext highlighter-rouge">squashfs + dm-verity</code>)</li>
      <li>For B2, N = 76171 (ext4 partition)</li>
    </ul>
  </li>
  <li>Average <a href="https://github.com/efarrer/blktrace/blob/d1927de948d47a85caf396b327b30a19ca711990/btt/doc/btt.tex#L144">D2C</a> time for B1 is more than twice that of B2 <code class="language-plaintext highlighter-rouge">(0.000176883 &gt; 2*0.000067955)</code>.</li>
</ul>

<p>D2C time: the average time from when the actual IO was issued to the driver until is completed (completion trace) back to the block IO layer.</p>

<p><code class="language-plaintext highlighter-rouge">--d2c-latencies</code> option: This option instructs <code class="language-plaintext highlighter-rouge">btt</code> to generate the D2C latency ﬁle. This file has the ﬁrst column (X values) representing runtime (seconds), while the second column (Y values) shows the actual latency for command at that time (either Q2D, D2C or Q2C).
<img src="https://notes.igalia.com/uploads/7eeac300-8cc6-4bd6-8b9d-295474469357.png" alt="" /></p>

<h3 id="fioflexible-io-tester">FIO(Flexible IO Tester)</h3>

<p>Up until now, all analysis has been performed using a <code class="language-plaintext highlighter-rouge">dd</code> which emulate sequential read requests.</p>

<p>Read access pattern also affects a IO performace so to emulated a random IO workloads tools like FIO is useful.</p>

<p>When you specify a <code class="language-plaintext highlighter-rouge">--size</code>, FIO creates a file with <code class="language-plaintext highlighter-rouge">filename</code> and when you don’ specify it FIO reads form a <code class="language-plaintext highlighter-rouge">filename</code>.</p>

<p>We are testing an existing chrome binary without providing its size.</p>

<p><code class="language-plaintext highlighter-rouge">localhost~# fio --name=4k_read --ioengine=libaio --iodepth=1 --rw=randread --bs=4k --norandommap --filename=&lt;dir&gt;/chrome</code></p>

<h3 id="hotspots">Hotspots</h3>

<p>A “hotspot” is a place where the app is spending a lot of time. After performing above analysis,  find those areas and speed them up. This is easily done using a profiling tool like
It has been found so far that there are differences between file systems, read request merge patterns, and IO schedulers. But there one more difference which we havent noticed yet.</p>

<p>For optimal performance,</p>

<ul>
  <li>reduce the number of requests (N)
    <ul>
      <li>by tuning the <em>blocksize</em> of the filesystem(squashfs) of the partition</li>
    </ul>
  </li>
  <li>reduce the time taken to process each request(D2C)
    <ul>
      <li>by tuning a compression algorithm used by squashfs filesystem</li>
      <li>by tuning a hashing function used by dm-verity</li>
    </ul>
  </li>
</ul>

<h4 id="tuning-a-blocksize">Tuning a blocksize</h4>

<p>Block is a continuous location on the hard drive where data is stored. In general, FileSystem stores data as a collection of blocks. When the block size is small, the data gets divided into blocks and distributed in more blocks. As more blocks are created, there will be more number of read requests.</p>

<p>Squashfs compresses files, inodes, and directories, and supports block sizes from 4 KiB up to 1 MiB for greater compression.</p>

<p>We experimented with different block sizes of 4K, 64K, 128K, 256K. As the block size is increasing, the read speed for DD is increasing. We have chosen 128K.</p>

<p><code class="language-plaintext highlighter-rouge">mksquashfs /tmp/squashfs-root /home/chronos/cros-components/lacros-dogfood-dev/100.0.4881.0/image.squash.test.&lt;block-size&gt; -b &lt;block-size&gt;K</code></p>

<h4 id="tuning-a-compression-algorithm">Tuning a compression algorithm</h4>

<p>Squashfs supports different compression algorithms. <code class="language-plaintext highlighter-rouge">mksquashfs</code> command has the option <code class="language-plaintext highlighter-rouge">-comp &lt;comp&gt;</code> to configure the compression algorithm. There are different compressors available:</p>

<ul>
  <li>gzip (default)</li>
  <li>lzma</li>
  <li>lzo</li>
  <li>lz4</li>
  <li>xz</li>
  <li>zstd</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">mksquashfs /tmp/squashfs-root /home/chronos/cros-components/lacros-dogfood-dev/100.0.4881.0/image.squash.test.&lt;block-size&gt; -b &lt;block-size&gt;K -comp gzip</code></p>

<h4 id="tuning-a-hashing-algorithm">Tuning a hashing algorithm</h4>

<p>This is related to dm-verity. <a href="https://chromium.googlesource.com/chromiumos/platform/dm-verity/+/refs/heads/main/verity_main.cc#18">dm-verity</a> decrypt the read request. It uses different hashing algorithms.</p>

<ul>
  <li>sha256 (default)</li>
  <li>sha224</li>
  <li>sha1</li>
  <li>md5
We can use hardware implementation of the above algorithms to speed up things.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>The purpose of this blog post is to demonstrate how to identify the hotspot and how to explore different parameters affecting the system’s performance. It was an interesting experience and I learned a lot. By sharing all the details about how the problem was broken down and improved, I hope others can learn something as well. As a result of playing around with the various combinations of parameters such as blocksize, compression, and direct-io, we have decided to go with the following configuration.</p>

<ol>
  <li>For Filesystem,
    <ul>
      <li>Blocksize : Changed from <code class="language-plaintext highlighter-rouge">4K</code> to <code class="language-plaintext highlighter-rouge">128K</code></li>
      <li>Compression algorithm : Changed from <code class="language-plaintext highlighter-rouge">gzip</code> to <code class="language-plaintext highlighter-rouge">xz</code></li>
      <li>direct-io : Changed from <code class="language-plaintext highlighter-rouge">on</code> to <code class="language-plaintext highlighter-rouge">off</code></li>
    </ul>
  </li>
  <li>dm-verity :
    <ul>
      <li>Hashing algorithm : Currently it used <code class="language-plaintext highlighter-rouge">sha256</code>. Either use hardware implementation of <code class="language-plaintext highlighter-rouge">sha256</code> or switch to other algorithm.</li>
    </ul>
  </li>
</ol>

<p><img src="https://notes.igalia.com/uploads/b0a8e7e7-8d1a-48bf-a295-060206faa5c0.png" alt="" /></p>

<p>Your first step should be to identify your current stack and then analyze each layer of your file system stack using the available tools and optimize them. The information provided in this blog may be helpful to you in deciding what to do about your own performance issues. You can get it touch with <a href="abhijeet@igalia.com">me</a> if you need any help.</p>

  </div>

  <div class="date">
    Written on November  2, 2022
  </div>

  
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="https://github.com/abhijeetk"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/abhijeetkandalkar"><i class="svg-icon linkedin"></i></a>


<a href="https://www.twitter.com/Abhijeet58"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    

  </body>
</html>
